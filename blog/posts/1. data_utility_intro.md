# Intoducing the Data Utility Architecture

The Data Utility Architecture (**DAU**) is a new way of thinking about enterprise architecture with data as _the_ first class citizen. This is in contrast to application first architectures (like microservices, monoliths, or event driven architectures) that have been the norm for the last decades. Rather than planning an enterprise architecture around applications that are built to achieve some goal, the DAU is about designing your architecture around where data comes from and needs to go.

The DAU is not so much a new architecture as a way to use existing ideas in a new way to think about enterprise architecture as your data architecture, bringing the analytical and operational data planes together, as Zhamak Dehghani suggested in her seminal article on [Data Mesh](https://martinfowler.com/articles/data-mesh-principles.html). The key components of a Data Utility are:

- Lakehouse
- Event sourced data
- Kappa architecture
- Medallion architecture
- Data as a product

I also think that using a Data Mesh approach to organizing your data teams is a good idea (and the DAU pulls heavily from the Data Mesh principles), but I don't think it's necessary to have a Data Mesh to have a Data Utility.

This article will introduce the DAU and explain how it works. Future articles will go into more detail on each of the components of the DAU.

## Traditional Application First Architecture

Typically, when we need to get something done and we believe technology can help, we start by thinking about what applications we need to build. We might start by thinking about what data we need to store, but we don't think about it in terms of the data itself, but rather in terms of the applications that will use the data. 

Whatever approach we take to designing applications we typically identify some equivalent of a bounded context for the application, which helps us identify the scope. We then design the application to meet the needs of the of the business and the users, working through the UX and UI. Separately we (hopefully) design the data model to meet the needs of the application. If the app needs data from other applications, we design the integration points and the data model for the data that will be shared.

In a well-architected enterprise, this new application will be integrated with other applications and data sources. We might create a set of APIs to expose the data to other applications, or we might use a message bus to publish events that other applications can subscribe to. We might even use a combination of both. This is, broadly speaking, a microservices architecture. Maybe the enterprise has been set up as a data mesh too, so that the domain data is made available as a product. 

Finally, our data team arrives to build the data pipelines to move data from the new source systems to the data lake(house) where it can be used by the data science team to build models and by the analytics team to build reports and dashboards.

This seems like a reasonable approach. The domain experts and the users of the application are the ones who know what the application needs to do, so they should be the ones to design it. The data team is there to make sure the data is available to the application and to the rest of the enterprise. The data science teams get access to quality data coming from data products. So why do we need a new approach?

### Reality of Application First Architecture

